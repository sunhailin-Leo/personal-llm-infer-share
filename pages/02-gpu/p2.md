# 用 A100 举的第二个🌰
所有的 SM 共享 L2 缓存。整个 GPU 内存结构如左图所示：

全局内存（也称为设备内存）是 GPU 内最大的内存，即常说的 HBM 内存。HBM 通过将多个 DRAM 芯片垂直堆叠实现高密度存储同时也满足高并行性和带宽。

L2 缓存可以被所有 SM 访问，速度比全局内存快；L1 缓存用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，**<font color="red">但跨 SM 之间的 L1 不能相互访问</font>**。

合理运用 L2 缓存能够提速运算。A100 的 L2 缓存能够设置至多 40MB 的持久化数据，能够提升算子 kernel 的带宽和性能。
