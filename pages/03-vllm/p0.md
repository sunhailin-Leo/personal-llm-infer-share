# 推理引擎之 vllm（开篇）
开篇废话~

vllm 是伯克利大学 LMSYS 组织开源的大语言模型高速推理框架，旨在极大地提升实时场景下的语言模型服务的吞吐与内存使用效率。

vllm 是一个快速且易于使用的库，用于大语言模型的推理场景，可以和 HuggingFace 无缝集成。

vllm 利用了全新的注意力算法「PagedAttention」，有效地管理注意力键和值。

<img src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png">
