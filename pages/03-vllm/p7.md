# 推理引擎之 vllm（七）
针对前面的缺点，目前的解决方案如下：

(Re)scheduling 开销
* 合理间隔调度：比如每 16 次 decode 计算后，检查一下是否有新的输入，以及是否有空闲的槽位，然后对 batch 做一次调度调整
* 排队比例调度：比如当前 batch 中有 10 个请求的 decode 正在进行，而排队中有 12 个请求，超过了排队比例 1.2，那么就启动一次调度调整

KV Cache 重读
* 在重调度时，如有新请求，将新请求 prefill 计算和要进行的 decode 计算做合并
* 在重调度时，如有新请求，对新请求做 prefill 计算，再合并所有进行中的请求做 decode 计算
* 根据 decode 耗时估算出来每次 decode 同时能做 prefill 的 token 数量
    * 在重调度时，如有新请求，对新请求 prefill 按照上面的估算进行分块，将分块的 prefill 和其它请求的 decode 计算融合在一起
        * 一定程度上减少了 KV Cache 重读的次数，又避免先做 prefill 计算带来的 Token 生成延时增加