# 推理引擎之 TensorRT-LLM（四）
TensorRT-LLM 优化点

优化部分对于 TRT-LLM 比 vllm 的优势在于下面几个方面：
* TensorRT Compiler：TRT 的 Graph Rewriting 功不可没
* Attention：
  * 用大量定制化 shape 来做 Kernel Selection 的操作，在编译计算图时选用
  * 定义 XQA（取代 MHA/MQA/GQA 等），利用 CUDA Core 和 Tensor Core “强行”将 GEMV（通用矩阵-向量乘法） 转化为 GEMM（通用矩阵-矩阵乘法）
* FFN（Feed-Forward Network）：基于 FT 积累的 cutlass 和 cuBLAS 的 cuda 函数极致提升 GEMM 性能
* In-flight Batching：又叫 Continuous Batching，实际推理效果会比 vllm 在资源利用率和调度开销上更好：
  * TRT-LLM 可以允许每个请求使用独立的 Decode Stream，而不是 vllm 那种全局 Stream
  * TRT-LLM 偏向于吞吐优先策略，最大化 GPU 利用率，而不是 vllm 的首 Token 策略
* KV-Cache：这块被 NV 闭源了，效率高也和定制的 cuda 函数相关

🥚彩蛋：可以看看 [SGLang v0.4 Blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/) 很有意思~  