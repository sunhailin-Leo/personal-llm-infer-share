# FlashAttention（三）
优化思路

传统的 Attention 计算存在 $O(N^2)$ 复杂度的矩阵对 HBM 内存及其重复读写是一个主要瓶颈
* 在不访问整个输入的情况下计算 softmax
* 不为反向传播存储大的中间 attention 矩阵

🔑：解决方案（Tiling 和 Recomputation）
* Tiling - 注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行 softmax 操作。
* Recomputation - 存储来自前向的 softmax 归一化因子，以便在反向中快速重新计算芯片上的 attention，这比从 HBM 读取中间矩阵的标准注意力方法更快。

该算法背后的主要思想是分割输入，将它们从慢速 HBM 加载到快速 SRAM，然后计算这些块的 attention 输出。在将每个块的输出相加之前，将其按正确的归一化因子进行缩放，从而得到正确的结果。