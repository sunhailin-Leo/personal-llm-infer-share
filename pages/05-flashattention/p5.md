# FlashAttention（五）
FA1 的问题

FlashAttention 存在一些效率低下的问题，由于 GPU 上不同线程块和 Warp 之间的工作划分不够理想，导致占用率低或不必要的共享内存读/写。

前向传播仅达到设备理论最大 FLOPs/s 的 30-50%，而反向传播更具挑战性，仅达到 A100 GPU 最大吞吐量的 25-35%，相比之下，优化后的矩阵乘法可以达到理论最大设备吞吐量的 80-90%

So, FlashAttention 2 is coming:
* 调整算法以减少非矩阵乘法操作的浮点运算次数，同时保持输出不变
* 在序列长度维度上同时并行化前向传播和反向传播
* 即使在注意力计算的一个块内部，我们也将工作分配给不同的线程块以减少通信和共享内存的读写

🔑：上面三点归结起来就是两个方向，第一个减少非矩阵计算量（与矩阵计算性能差异有 16 倍之多），第二部分就是 GEMM 层面优化。

接下来稍微具体看看 FA2 是怎么改进的~