# FlashAttention（九）
Forward 和 Backward Pass 的优化点

在 V1 中，按 batch_size 和 num_heads 来划分 block 的；
而 V2 中，按 batch_size，num_heads 和 num_m_block 来划分 block 的，其中 num_m_block 可理解成是沿着 Q 矩阵行方向做的切分。

目的是尽量让 SM 单元打满（提高 GPU 利用率）

<div style="display: flex; justify-content: space-between; align-items: center; width: 770px; margin: 0 auto;">
    <img src="https://picx.zhimg.com/v2-43d5a80298fd68859bcfa8752abf6e63_1440w.jpg" style="width: 49.8%;"/>
    <img src="https://pic2.zhimg.com/v2-cba1ea014b1422b1c0701c93cc36273d_1440w.jpg" style="width: 49.8%;"/>
</div>